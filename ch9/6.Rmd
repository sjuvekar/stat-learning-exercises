Chapter 9: Exercise 6
========================================================

### a
We randomly generate 1000 points and scatter them across line $x = y$. 
```{r 6a}
set.seed(3154)
x = runif(1000, 0, 100)
class.one = sample(1000, 500)
y = rep(NA, 1000)
# Set y > x for class.one
for(i in class.one) {
  y[i] = runif(1, x[i], 100)
}
# set y < x for class.zero
for (i in setdiff(1:1000, class.one)) {
  y[i] = runif(1, 0, x[i])
}
plot(x[class.one], y[class.one], col="blue", pch="+")
points(x[-class.one], y[-class.one], col="red", pch=4)
```
The plot shows that classes are barely separable. In fact, some points practically lie on the decision boundary.

### b
We create a z variable according to classes.
```{r}
library(e1071)
set.seed(555)
z = rep(0, 1000)
z[class.one] = 1
data = data.frame(x=x, y=y, z=z)
tune.out = tune(svm, as.factor(z)~., data=data, kernel="linear", ranges=list(cost=c(0.01, 0.1, 1, 5, 10, 100, 1000, 10000)))
summary(tune.out)
data.frame(cost=tune.out$performances$cost, misclass=tune.out$performances$error * 1000)
```
The table above shows train-misclassification error for all costs. A cost of 1000 seems to classify all points correctly. This also corresponds to a cross-validation error of 0.

### c
We now generate a random test-set of same size. We also 'violate' the boundary for some points, i.e., make then cross the boundary.
```{r 6c}
set.seed(1111)
x.test = runif(1000, 0, 100)
class.one = sample(1000, 500)
y.test = rep(NA, 1000)
# Set y > x for class.one
for(i in class.one) {
  y.test[i] = runif(1, x.test[i]-1, 100)
}
# set y < x for class.zero
for (i in setdiff(1:1000, class.one)) {
  y.test[i] = runif(1, 0, x.test[i]+1)
}
plot(x.test[class.one], y.test[class.one], col="blue", pch="+")
points(x.test[-class.one], y.test[-class.one], col="red", pch=4)
```
We now make same predictions using all linear svms with all costs used in previous part. 
```{r}
set.seed(30012)
z.test = rep(0, 1000)
z.test[class.one] = 1
all.costs = c(0.01, 0.1, 1, 5, 10, 100, 1000, 10000)
test.errors = rep(NA, 8)
data.test = data.frame(x=x.test, y=y.test, z=z.test)
for (i in 1:length(all.costs)) {
  svm.fit = svm(as.factor(z)~., data=data, kernel="linear", cost=all.costs[i])
  svm.predict = predict(svm.fit, data.test)
  test.errors[i] = sum(svm.predict != data.test$z)
}

data.frame(cost=all.costs, "test misclass"=test.errors)
```
$\tt{cost} = 10$ seems to be performing better on test data, making the least number of classification errors. This is much smaller than optimal value of 1000 for training data.

### d
We again see an overfitting phenomenon for linear kernel. A large cost allows more flexible model and hence fits the train data hard. A small cost, however, makes a few errors on both train and test data.